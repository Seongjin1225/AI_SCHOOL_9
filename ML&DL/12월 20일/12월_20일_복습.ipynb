{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMXI+yMtR9vAAc4gqr8xGET",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Seongjin1225/AI_SCHOOL_9/blob/main/ML%26DL/12%EC%9B%94%2020%EC%9D%BC/12%EC%9B%94_20%EC%9D%BC_%EB%B3%B5%EC%8A%B5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PAYN1DukEC0k"
      },
      "outputs": [],
      "source": [
        "# IMDB Dataset Load\n",
        "from tensorflow.keras.datasets import imdb\n",
        "(x_data_train, t_data_train), (x_data_test, t_data_test) = \\\n",
        "imdb.load_data(num_words=500)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "x_data_train_seq = pad_sequences(x_data_train, maxlen=100)\n",
        "x_data_train_seq.shape\n",
        "\n",
        "x_data_train_seq[0]\n",
        "\n",
        "# 결국 one-hot encoding으로 처리해야 해요\n",
        "# 하나하나의 token이 500개짜리 1차원 ndarray로 표현되어야\n",
        "# 전체 -> (100,500)\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "x_data_train_onehot = to_categorical(x_data_train_seq)\n",
        "x_data_train_onehot.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqDNYR7eFD0s",
        "outputId": "0e2da927-13f2-48ab-98f7-f7ac41ee8e15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 100, 500)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## RNN 모델 구현\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(units=8,\n",
        "                    activation='tanh',\n",
        "                    input_shape=(100,500)))\n",
        "model.add(Dense(units=1,\n",
        "                activation='sigmoid'))\n"
      ],
      "metadata": {
        "id": "WSOQyADGFu99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 전체 코드로 작성\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# 1. data loading\n",
        "(x_data_train, t_data_train) , (x_data_test, t_data_test) = \\\n",
        "imdb.load_data(num_words=500)\n",
        "\n",
        "# 2. 데이터 분리(test는 위에 있었으므로 valid data)\n",
        "x_data_train, x_data_valid, t_data_train, t_data_valid = \\\n",
        "train_test_split(x_data_train,\n",
        "                 t_data_train,\n",
        "                 test_size=0.2,\n",
        "                 stratify=t_data_train,\n",
        "                 random_state=42)\n",
        "\n",
        "# review의 길이 통일\n",
        "x_data_train_seq = pad_sequences(x_data_train, maxlen=100)\n",
        "x_data_valid_seq = pad_sequences(x_data_valid, maxlen=100)\n",
        "\n",
        "# one-hot 형태로 변형\n",
        "x_data_train_onehot = to_categorical(x_data_train_seq)\n",
        "x_data_valid_onehot = to_categorical(x_data_valid_seq)\n",
        "\n",
        "# 모델 생성\n",
        "model = Sequential()\n",
        "\n",
        "model.add(SimpleRNN(units=8,\n",
        "                    activation='tanh',\n",
        "                    input_shape=(100,500)))\n",
        "\n",
        "model.add(Dense(units=1,\n",
        "                activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=1e-4),\n",
        "              loss = 'binary_crossentropy',\n",
        "              metrics=['acc'])\n",
        "\n",
        "es_cb = EarlyStopping(monitor='val_loss',\n",
        "                      patience=4,\n",
        "                      restore_best_weights=True)\n",
        "\n",
        "cp_cb = ModelCheckpoint(filepath='./best-simplernn-model.ckpt',\n",
        "                        monitor='val_loss',\n",
        "                        verbose=1,\n",
        "                        save_best_only=True)\n",
        "\n",
        "history = model.fit(x_data_train_onehot,\n",
        "                    t_data_train,\n",
        "                    epochs=100,\n",
        "                    verbose=1,\n",
        "                    batch_size=64,\n",
        "                    validation_data = (x_data_valid_onehot, t_data_valid),\n",
        "                    callbacks=[es_cb, cp_cb])"
      ],
      "metadata": {
        "id": "TRCVm4J0PsTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 시각화\n",
        "\n",
        "train_loss = history.history['loss']\n",
        "valid_loss = history.history['val_loss']\n",
        "\n",
        "plt.plot(train_loss, 'o', color='r')\n",
        "plt.plot(valid_loss, color='b')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7vA6lt0uT5nD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# one-hot 방식으로 하면 데이터의 양도 많아지고, 시간도 오래 걸림!\n",
        "# word embedding 방식으로 구현\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, Dense, Embedding\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# 1. data loading\n",
        "(x_data_train, t_data_train) , (x_data_test, t_data_test) = \\\n",
        "imdb.load_data(num_words=500)\n",
        "\n",
        "# 2. 데이터 분리(test는 위에 있었으므로 valid data)\n",
        "x_data_train, x_data_valid, t_data_train, t_data_valid = \\\n",
        "train_test_split(x_data_train,\n",
        "                 t_data_train,\n",
        "                 test_size=0.2,\n",
        "                 stratify=t_data_train,\n",
        "                 random_state=42)\n",
        "\n",
        "# review의 길이 통일\n",
        "x_data_train_seq = pad_sequences(x_data_train, maxlen=100)\n",
        "x_data_valid_seq = pad_sequences(x_data_valid, maxlen=100)\n",
        "\n",
        "# 모델 생성\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(input_dim=500,  # vocabulary 수\n",
        "                    output_dim=16,  # embedding size - 칸의 개수(one-hot의 경우 500개였음)\n",
        "                    input_length=100))  # time-stamp(길이?)\n",
        "\n",
        "model.add(SimpleRNN(units=8,\n",
        "                    activation='tanh',\n",
        "                    input_shape=(100,500)))\n",
        "\n",
        "model.add(Dense(units=1,\n",
        "                activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=1e-4),\n",
        "              loss = 'binary_crossentropy',\n",
        "              metrics=['acc'])\n",
        "\n",
        "es_cb = EarlyStopping(monitor='val_loss',\n",
        "                      patience=4,\n",
        "                      restore_best_weights=True)\n",
        "\n",
        "cp_cb = ModelCheckpoint(filepath='./best-simplernn-model.ckpt',\n",
        "                        monitor='val_loss',\n",
        "                        verbose=1,\n",
        "                        save_best_only=True)\n",
        "\n",
        "history = model.fit(x_data_train_seq,\n",
        "                    t_data_train,\n",
        "                    epochs=100,\n",
        "                    verbose=1,\n",
        "                    batch_size=64,\n",
        "                    validation_data = (x_data_valid_seq, t_data_valid),\n",
        "                    callbacks=[es_cb, cp_cb])"
      ],
      "metadata": {
        "id": "lin_xBkrT9T3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "sentences = [\n",
        "    '강감찬은 나를 너무 너무 좋아해',\n",
        "    '강감찬은 영화를 좋아해'\n",
        "]\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)  # 단어사전 생성\n",
        "\n",
        "tokenizer.word_index\n",
        "\n",
        "word_encoding = tokenizer.texts_to_sequences(sentences)\n",
        "word_encoding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HtiiLHkcU7tU",
        "outputId": "61a47548-76f1-4d7b-84b8-25fd587284a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 4, 2, 2, 3], [1, 5, 3]]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어사전에 없는 새로운 단어는??  -> 그냥 없애버리는 문제 발생\n",
        "new_sentence = [\n",
        "    '강감찬은 신사임당을 너무 좋아해'\n",
        "]\n",
        "new_word_encoding = tokenizer.texts_to_sequences(new_sentence)\n",
        "new_word_encoding\n",
        "\n",
        "# 해결방법?\n",
        "# OOV(out of vocabulary) token으로 처리\n",
        "tokenizer = Tokenizer(oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "print(tokenizer.word_index)\n",
        "\n",
        "new_word_encoding = tokenizer.texts_to_sequences(new_sentence)\n",
        "new_word_encoding\n",
        "\n",
        "# 길이도 맞춰보기\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "padding = pad_sequences(word_encoding, maxlen=4)\n",
        "padding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hi2nBbToVOuj",
        "outputId": "f39277e7-79ff-41ce-eb2f-7451469d5905"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'<OOV>': 1, '강감찬은': 2, '너무': 3, '좋아해': 4, '나를': 5, '영화를': 6}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[4, 2, 2, 3],\n",
              "       [0, 1, 5, 3]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Naver 영화 리뷰 댓글 분석\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "# 데이터 가져오기\n",
        "train_file = tf.keras.utils.get_file(\n",
        "    '/content/ratings_train.txt',\n",
        "    origin='https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt',\n",
        "    extract=True\n",
        ")\n",
        "train_df = pd.read_csv(train_file, sep='\\t')\n",
        "train_df\n",
        "\n",
        "test_file = tf.keras.utils.get_file(\n",
        "    '/content/ratings_test.txt',  # 파일 저장 경로\n",
        "    origin = 'https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt',  # origin => 데이터 읽어올 경로\n",
        "    extract=True\n",
        ")\n",
        "test_df = pd.read_csv(test_file, sep='\\t')\n",
        "test_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcK8LdyIVnKh",
        "outputId": "8205d9d9-f725-43e4-d28b-fd602fe111e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\n",
            "4893335/4893335 [==============================] - 0s 0us/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 한글 형태소 분석\n",
        "# !pip install konlpy\n",
        "\n",
        "from konlpy.tag import Okt, Kkma, Komoran\n",
        "\n",
        "kkma = Kkma()\n",
        "okt = Okt()\n",
        "komoran = Komoran()\n",
        "\n",
        "text = '이것은 소리없는 아우성. 저 푸른 해원을 향하여 흔드는 노스텔지어의 손수건'\n",
        "\n",
        "kkma.morphs(text)\n",
        "# ['이것', '은', '소리', '없', '는', '아우성', '.', '저', '푸른',\n",
        "# '해원', '을', '향하', '여', '흔들', '는', '노스텔지어', '의', '손수건']\n",
        "\n",
        "okt.morphs(text)\n",
        "# ['이', '것', '은', '소리', '없는', '아우성', '.', '저', '푸른', '해원',\n",
        "# '을', '향', '하여', '흔드는', '노스', '텔', '지', '어의', '손수건']\n",
        "\n",
        "komoran.morphs(text)\n",
        "# ['이것', '은', '소리', '없', '는', '아우성', '.', '저', '푸른', '해원',\n",
        "# '을', '향하', '아', '흔들', '는', '노스', '텔', '짓', '어', '의', '손수건']"
      ],
      "metadata": {
        "id": "-BXTXvVdW4MM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 전처리\n",
        "# 1. 특수문자 제거(영문, 한글, 띄어쓰기 제외 삭제)\n",
        "# 2. 결측치 제거\n",
        "# 3. 불용어(stop word) 처리\n",
        "# 4. 단어사전 생성 & 숫자로 변경하는 token화 작업\n",
        "# 5. 동일한 문장 길이 설정\n",
        "\n",
        "#################################################\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "# Naver 영화 리뷰 데이터를 로딩\n",
        "train_file = tf.keras.utils.get_file(\n",
        "    '/content/ratings_train.txt',\n",
        "    origin='https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt',\n",
        "    extract=True\n",
        ")\n",
        "train_df = pd.read_csv(train_file, sep='\\t')\n",
        "\n",
        "test_file = tf.keras.utils.get_file(\n",
        "    '/content/ratings_test.txt',\n",
        "    origin='https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt',\n",
        "    extract=True\n",
        ")\n",
        "test_df = pd.read_csv(test_file, sep='\\t')\n",
        "\n",
        "# 1. 정규표현식 처리(^ == not)\n",
        "train_df['document'] = train_df['document'].str.replace('[^A-Za-z가-힣ㄱ-ㅎㅏ-ㅣ ]','')\n",
        "train_df.head()\n",
        "\n",
        "# 2. 결측치 처리\n",
        "train_df = train_df.dropna()\n",
        "train_df.isnull().sum()\n",
        "\n",
        "# 3. 불용어(stop word) 처리\n",
        "# 관사, 전치사, 조사, 접속사등의 의미가 없는 단어 -> 불용어\n",
        "# 여기서는 자주사용하는거 몇개만 처리\n",
        "from konlpy.tag import Okt\n",
        "okt = Okt()\n",
        "\n",
        "def word_tokenization(text):\n",
        "    stop_words=  ['는', '을', '를', '이', '가', '의', '던', '고', '하', '다',\n",
        "                  '은', '에', '들', '지', '게', '도']\n",
        "    return [x for x in okt.morphs(text) if x not in stop_words]\n",
        "\n",
        "data_train = train_df['document'].apply(lambda x: word_tokenization(x))\n",
        "data_train"
      ],
      "metadata": {
        "id": "ZVgb6EIQXuie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 불용어 처리가 진행됬다고 가정하고 진행\n",
        "# 4. 단어사전 만들고 문자 -> 숫자\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(data_train)\n",
        "\n",
        "# 총 단어 수 확인\n",
        "len(tokenizer.word_index)\n",
        "\n",
        "# 각 토큰의 빈도수\n",
        "tokenizer.word_counts\n",
        "\n",
        "# 각 토큰의 빈도수를 숫자만 출력\n",
        "tokenizer.word_counts.values()\n",
        "\n",
        "# 단어 빈도수 5회 이상인 것만\n",
        "def get_voca_size(threshold):\n",
        "    count = 0\n",
        "    for num in tokenizer.word_counts.values():\n",
        "        if num > 5:\n",
        "            count += 1\n",
        "    return count\n",
        "\n",
        "voca_size = get_voca_size(5)\n",
        "\n",
        "# 단어사전 생성\n",
        "tokenizer = Tokenizer(oov_token='<OOV>',\n",
        "                      num_words=15000)\n",
        "\n",
        "tokenizer.fit_on_texts(data_train)\n",
        "\n",
        "# 각 문장을 숫자 벡터로 변환하여 encoding\n",
        "data_train_seq = tokenizer.texts_to_sequences(data_train)\n",
        "\n",
        "\n",
        "# 길이 다 맞춰주기\n",
        "# 최대 길이 구하기\n",
        "max_length = max([len(x) for x in data_train_seq])\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_seqeuces\n",
        "x_data_train = pad_sequences(data_train_seq,\n",
        "                             maxlen=max_length)\n",
        "t_data_train = train_df['label'].values"
      ],
      "metadata": {
        "id": "wY0BmQlaZYcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 생성\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding, Bidirectional\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(input_dim=15000,\n",
        "                    output_dim=32,\n",
        "                    input_length=71))\n",
        "\n",
        "model.add(Bidirectional(LSTM(units=16,\n",
        "               activation='tanh')))\n",
        "\n",
        "# hidden layer\n",
        "model.add(Dense(units=32,\n",
        "                activation='relu'))\n",
        "\n",
        "# output layer\n",
        "model.add(Dense(units=1,\n",
        "                activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate = 1e-4),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['acc'])\n",
        "\n",
        "# callback 설정\n",
        "es_cb = EarlyStopping(monitor='val_loss',\n",
        "                      patience=3,\n",
        "                      restore_best_weights=True)\n",
        "\n",
        "cp_cb = ModelCheckpoint(filepath='./best_model.ckpt',\n",
        "                        save_weights_only=True,\n",
        "                        save_best_only=True,\n",
        "                        monitor='val_loss',\n",
        "                        verbose=1)\n",
        "\n",
        "# 학습\n",
        "history = model.fit(x_data_train,\n",
        "                    t_data_trian,\n",
        "                    epochs=10,\n",
        "                    verbose=1,\n",
        "                    validation_split=0.2,\n",
        "                    batch_size=64,\n",
        "                    callbacks=[es_cb, cp_cb])"
      ],
      "metadata": {
        "id": "3E0zBzCHnBbl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습이 끝난다음 예측\n",
        "import pandas as pd\n",
        "review_sentences = ['내가 만들어도 이것보단 잘 만들겠다.',\n",
        "                    '너무너무 재미있었습니다. 감사합니다.',\n",
        "                    '아..내 돈... 돈 아까워 죽을거 같아요',\n",
        "                    '아나..이것도 영화라고 만들었냐. 무슨 스토리가 산으로 가냐',\n",
        "                    '감동과 재미가 같이 있는 영화입니다. 훌륭합니다.',\n",
        "                    '너무너무 재미없다.. 잠와 죽는줄.',\n",
        "                    '너무너무 재미있다.. 잠이 확깨네.']\n",
        "\n",
        "df = pd.DataFrame({'document':review_sentences})\n",
        "df\n",
        "\n",
        "# 정규표현식 사용해 특수분자 제거\n",
        "df['document'] = df['document'].str.replace('[^A-Za-z가-힣ㄱ-ㅎㅏ-ㅣ ]','')\n",
        "df\n",
        "\n",
        "# 결측치 제거는 없으니까 생략\n",
        "\n",
        "# 불용어 처리\n",
        "from konlpy.tag import Okt\n",
        "okt = Okt()\n",
        "\n",
        "def stop_word(text):\n",
        "    stop_word = ['는', '을', '를', '이', '가', '의', '던', '고', '하', '다',\n",
        "                  '은', '에', '들', '지', '게', '도']\n",
        "\n",
        "    return [x for x in okt.morphs(text) if x not in stop_word]\n",
        "\n",
        "data_predict = df['document'].apply(lambda x : stop_word(x))\n",
        "\n",
        "# 각 문장 숫자 벡터로 변환\n",
        "data_predict_seq = tokenizer.texts_to_sequences(data_predict)\n",
        "\n",
        "# 길이 통일\n",
        "x_data_predict = pad_sequences(data_predict_seq,\n",
        "                               truncating='post',  # maxlen 초과 시 앞에 잘라냄 의미\n",
        "                               padding='pre',      # padding 시 앞에 채워넣는다 의미\n",
        "                               maxlen=max_length)\n",
        "\n",
        "# 예측\n",
        "print(model.predict(x_data_predict))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tybS2Faap-5D",
        "outputId": "d2b063b3-bc8c-412e-fe53-835fe5e1e607"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-37-b688165b532d>:15: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  df['document'] = df['document'].str.replace('[^A-Za-z가-힣ㄱ-ㅎㅏ-ㅣ ]','')\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "document    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bThf7XugqMSP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}