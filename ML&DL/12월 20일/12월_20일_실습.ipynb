{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMdlhJ2CCD21F5f2w4lJAwB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Seongjin1225/AI_SCHOOL_9/blob/main/ML%26DL/12%EC%9B%94%2020%EC%9D%BC/12%EC%9B%94_20%EC%9D%BC_%EC%8B%A4%EC%8A%B5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2TdJj7alDFY",
        "outputId": "49bc739a-472c-4c45-e7d6-0b836f467f17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17464789/17464789 [==============================] - 2s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# IMDB Review Dataset Loading\n",
        "from tensorflow.keras.datasets import imdb\n",
        "(x_data_train, t_data_train), (x_data_test, t_data_test) = \\\n",
        "imdb.load_data(num_words=500)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_data_train.shape\n",
        "t_data_train.shape\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "x_data_train_seq = pad_sequences(x_data_train,maxlen=100)\n",
        "# 모든 review의 길이가 100개의 token으로 획일화\n",
        "# 각 review에서 100개가 넘어가는 token은 삭제, 모자라는 token은 padding 처리\n",
        "\n",
        "# 아래 input_shpape이 100,500인 이유??\n",
        "# 결국은 one-hot encoding으로 처리해야\n",
        "# 하나하나의 token이 500개짜리 1차원 ndarray로 표현\n",
        "# 전체 데이터는 (100,500)\n",
        "\n",
        "# one-hot enconding 과\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "x_data_train_onehot = to_categorical(x_data_train_seq)\n",
        "x_data_train_onehot.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3aHvNkcmKIL",
        "outputId": "797e9795-1682-49da-dcd2-0af2cd6a3195"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 100, 500)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RNN Model 구현\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(units=8,\n",
        "                    activation='tanh',\n",
        "                    input_shape=(100,500)))\n",
        "\n",
        "# output layer\n",
        "model.add(Dense(units=1,\n",
        "                activation='sigmoid'))\n",
        "\n",
        "#"
      ],
      "metadata": {
        "id": "qQZZelQwmRVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 전체 코드 작성\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split  # 여기서는 train과 validation 분리하는 용도로 사용!\n",
        "\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# 1. Data Loading\n",
        "(x_data_train, t_data_train), (x_data_test, t_data_test) = \\\n",
        "imdb.load_data(num_words=500)\n",
        "\n",
        "# 2. 데이터 확인\n",
        "x_data_train.shape\n",
        "\n",
        "# 3. data 분리\n",
        "x_data_train, x_data_valid, t_data_train, t_data_valid = \\\n",
        "train_test_split(x_data_train,\n",
        "                 t_data_train,\n",
        "                 test_size=0.2,\n",
        "                 random_state=42,\n",
        "                 stratify=t_data_train)\n",
        "\n",
        "# review 길이 통일\n",
        "x_data_train_seq = pad_sequences(x_data_train, maxlen=100)\n",
        "x_data_valid_seq = pad_sequences(x_data_valid, maxlen=100)\n",
        "\n",
        "# x_data one-hot 형태로 변형해야 입력으로 사용 가능\n",
        "x_data_train_onehot = to_categorical(x_data_train_seq)\n",
        "x_data_valid_onehot = to_categorical(x_data_valid_seq)"
      ],
      "metadata": {
        "id": "LleMLEBSxqjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model 구현\n",
        "model = Sequential()\n",
        "\n",
        "model.add(SimpleRNN(units=8,\n",
        "                    activation='tanh',\n",
        "                    input_shape=(100,500)))\n",
        "\n",
        "model.add(Dense(units=1,\n",
        "                activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=1e-4),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['acc'])\n",
        "\n",
        "es_cb = EarlyStopping(monitor='val_loss',\n",
        "                      patience=4,\n",
        "                      verbose=1,\n",
        "                      restore_best_weights=True)\n",
        "\n",
        "cp_cb = ModelCheckpoint(filepath='./best-simplernn-model.ckpt',\n",
        "                        monitor='val_loss',\n",
        "                        verbose=1,\n",
        "                        save_best_only=True)\n",
        "\n",
        "history = model.fit(x_data_train_onehot,\n",
        "                    t_data_train,\n",
        "                    epochs=100,\n",
        "                    batch_size=64,\n",
        "                    validation_data = (x_data_valid_onehot,t_data_valid),\n",
        "                    callbacks=[es_cb, cp_cb])"
      ],
      "metadata": {
        "id": "NWhdk4Wx0yGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss = history.history['loss']\n",
        "valid_loss = history.history['val_loss']\n",
        "\n",
        "plt.plot(train_loss, color='r')\n",
        "plt.plot(valid_loss, color='b')"
      ],
      "metadata": {
        "id": "DXd5a6J95gcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# one-hot encoding 방식이 아니라 word embedding 방식으로\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split  # 여기서는 train과 validation 분리하는 용도로 사용!\n",
        "\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, Dense, Embedding\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# 1. Data Loading\n",
        "(x_data_train, t_data_train), (x_data_test, t_data_test) = \\\n",
        "imdb.load_data(num_words=500)\n",
        "\n",
        "# 2. 데이터 확인\n",
        "x_data_train.shape\n",
        "\n",
        "# 3. data 분리\n",
        "x_data_train, x_data_valid, t_data_train, t_data_valid = \\\n",
        "train_test_split(x_data_train,\n",
        "                 t_data_train,\n",
        "                 test_size=0.2,\n",
        "                 random_state=42,\n",
        "                 stratify=t_data_train)\n",
        "\n",
        "# review 길이 통일\n",
        "x_data_train_seq = pad_sequences(x_data_train, maxlen=100)\n",
        "x_data_valid_seq = pad_sequences(x_data_valid, maxlen=100)\n",
        "\n",
        "# Model 구현\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(input_dim=500,  # vocabulary 수\n",
        "                    output_dim=16,  # embedding size - 칸의 개수(one-hot의 경우 500개였음)\n",
        "                    input_length=100))  # time-stamp(길이?)\n",
        "\n",
        "model.add(SimpleRNN(units=8,\n",
        "                    activation='tanh'))\n",
        "\n",
        "model.add(Dense(units=1,\n",
        "                activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=1e-4),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['acc'])\n",
        "\n",
        "es_cb = EarlyStopping(monitor='val_loss',\n",
        "                      patience=4,\n",
        "                      verbose=1,\n",
        "                      restore_best_weights=True)\n",
        "\n",
        "cp_cb = ModelCheckpoint(filepath='./best-simplernn-model.ckpt',\n",
        "                        monitor='val_loss',\n",
        "                        verbose=1,\n",
        "                        save_best_only=True)\n",
        "\n",
        "history = model.fit(x_data_train_seq,\n",
        "                    t_data_train,\n",
        "                    epochs=100,\n",
        "                    batch_size=64,\n",
        "                    validation_data = (x_data_valid_seq,t_data_valid),\n",
        "                    callbacks=[es_cb, cp_cb])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rry6QBy2B33q",
        "outputId": "56f986bf-67c6-476d-87d0-9e8a9cd0d171"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 0.6675 - acc: 0.5915\n",
            "Epoch 1: val_loss improved from inf to 0.62218, saving model to ./best-simplernn-model.ckpt\n",
            "313/313 [==============================] - 42s 122ms/step - loss: 0.6675 - acc: 0.5915 - val_loss: 0.6222 - val_acc: 0.6808\n",
            "Epoch 2/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 0.5955 - acc: 0.7106\n",
            "Epoch 2: val_loss improved from 0.62218 to 0.58054, saving model to ./best-simplernn-model.ckpt\n",
            "313/313 [==============================] - 31s 99ms/step - loss: 0.5955 - acc: 0.7106 - val_loss: 0.5805 - val_acc: 0.7240\n",
            "Epoch 3/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 0.5600 - acc: 0.7485\n",
            "Epoch 3: val_loss improved from 0.58054 to 0.55860, saving model to ./best-simplernn-model.ckpt\n",
            "313/313 [==============================] - 31s 99ms/step - loss: 0.5600 - acc: 0.7485 - val_loss: 0.5586 - val_acc: 0.7446\n",
            "Epoch 4/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 0.5364 - acc: 0.7691\n",
            "Epoch 4: val_loss improved from 0.55860 to 0.54329, saving model to ./best-simplernn-model.ckpt\n",
            "313/313 [==============================] - 32s 103ms/step - loss: 0.5364 - acc: 0.7691 - val_loss: 0.5433 - val_acc: 0.7548\n",
            "Epoch 5/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 0.5198 - acc: 0.7796\n",
            "Epoch 5: val_loss improved from 0.54329 to 0.52840, saving model to ./best-simplernn-model.ckpt\n",
            "313/313 [==============================] - 30s 96ms/step - loss: 0.5198 - acc: 0.7796 - val_loss: 0.5284 - val_acc: 0.7620\n",
            "Epoch 6/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 0.5040 - acc: 0.7890\n",
            "Epoch 6: val_loss improved from 0.52840 to 0.52079, saving model to ./best-simplernn-model.ckpt\n",
            "313/313 [==============================] - 30s 97ms/step - loss: 0.5040 - acc: 0.7890 - val_loss: 0.5208 - val_acc: 0.7580\n",
            "Epoch 7/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 0.4932 - acc: 0.7917\n",
            "Epoch 7: val_loss improved from 0.52079 to 0.51595, saving model to ./best-simplernn-model.ckpt\n",
            "313/313 [==============================] - 31s 100ms/step - loss: 0.4932 - acc: 0.7917 - val_loss: 0.5160 - val_acc: 0.7632\n",
            "Epoch 8/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 0.4837 - acc: 0.7951\n",
            "Epoch 8: val_loss improved from 0.51595 to 0.50420, saving model to ./best-simplernn-model.ckpt\n",
            "313/313 [==============================] - 31s 98ms/step - loss: 0.4837 - acc: 0.7951 - val_loss: 0.5042 - val_acc: 0.7676\n",
            "Epoch 9/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 0.4747 - acc: 0.7994\n",
            "Epoch 9: val_loss improved from 0.50420 to 0.49949, saving model to ./best-simplernn-model.ckpt\n",
            "313/313 [==============================] - 31s 99ms/step - loss: 0.4747 - acc: 0.7994 - val_loss: 0.4995 - val_acc: 0.7696\n",
            "Epoch 10/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 0.4655 - acc: 0.8030\n",
            "Epoch 10: val_loss improved from 0.49949 to 0.49382, saving model to ./best-simplernn-model.ckpt\n",
            "313/313 [==============================] - 31s 99ms/step - loss: 0.4655 - acc: 0.8030 - val_loss: 0.4938 - val_acc: 0.7698\n",
            "Epoch 11/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 0.4581 - acc: 0.8054\n",
            "Epoch 11: val_loss improved from 0.49382 to 0.49088, saving model to ./best-simplernn-model.ckpt\n",
            "313/313 [==============================] - 30s 95ms/step - loss: 0.4581 - acc: 0.8054 - val_loss: 0.4909 - val_acc: 0.7716\n",
            "Epoch 12/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 0.4521 - acc: 0.8073\n",
            "Epoch 12: val_loss improved from 0.49088 to 0.48574, saving model to ./best-simplernn-model.ckpt\n",
            "313/313 [==============================] - 31s 100ms/step - loss: 0.4521 - acc: 0.8073 - val_loss: 0.4857 - val_acc: 0.7714\n",
            "Epoch 13/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 0.4463 - acc: 0.8099\n",
            "Epoch 13: val_loss did not improve from 0.48574\n",
            "313/313 [==============================] - 28s 89ms/step - loss: 0.4463 - acc: 0.8099 - val_loss: 0.4865 - val_acc: 0.7728\n",
            "Epoch 14/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 0.4410 - acc: 0.8142\n",
            "Epoch 14: val_loss improved from 0.48574 to 0.48451, saving model to ./best-simplernn-model.ckpt\n",
            "313/313 [==============================] - 29s 94ms/step - loss: 0.4410 - acc: 0.8142 - val_loss: 0.4845 - val_acc: 0.7708\n",
            "Epoch 15/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 0.4360 - acc: 0.8158\n",
            "Epoch 15: val_loss improved from 0.48451 to 0.48251, saving model to ./best-simplernn-model.ckpt\n",
            "313/313 [==============================] - 33s 105ms/step - loss: 0.4360 - acc: 0.8158 - val_loss: 0.4825 - val_acc: 0.7726\n",
            "Epoch 16/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 0.4330 - acc: 0.8151\n",
            "Epoch 16: val_loss improved from 0.48251 to 0.47819, saving model to ./best-simplernn-model.ckpt\n",
            "313/313 [==============================] - 29s 93ms/step - loss: 0.4330 - acc: 0.8151 - val_loss: 0.4782 - val_acc: 0.7768\n",
            "Epoch 17/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 0.4302 - acc: 0.8184\n",
            "Epoch 17: val_loss improved from 0.47819 to 0.47672, saving model to ./best-simplernn-model.ckpt\n",
            "313/313 [==============================] - 30s 96ms/step - loss: 0.4302 - acc: 0.8184 - val_loss: 0.4767 - val_acc: 0.7764\n",
            "Epoch 18/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 0.4263 - acc: 0.8181\n",
            "Epoch 18: val_loss did not improve from 0.47672\n",
            "313/313 [==============================] - 30s 96ms/step - loss: 0.4263 - acc: 0.8181 - val_loss: 0.4799 - val_acc: 0.7752\n",
            "Epoch 19/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 0.4228 - acc: 0.8195\n",
            "Epoch 19: val_loss improved from 0.47672 to 0.47243, saving model to ./best-simplernn-model.ckpt\n",
            "313/313 [==============================] - 32s 103ms/step - loss: 0.4228 - acc: 0.8195 - val_loss: 0.4724 - val_acc: 0.7782\n",
            "Epoch 20/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 0.4201 - acc: 0.8216\n",
            "Epoch 20: val_loss did not improve from 0.47243\n",
            "313/313 [==============================] - 30s 96ms/step - loss: 0.4201 - acc: 0.8216 - val_loss: 0.4772 - val_acc: 0.7764\n",
            "Epoch 21/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 0.4164 - acc: 0.8236\n",
            "Epoch 21: val_loss did not improve from 0.47243\n",
            "313/313 [==============================] - 31s 99ms/step - loss: 0.4164 - acc: 0.8236 - val_loss: 0.4749 - val_acc: 0.7790\n",
            "Epoch 22/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 0.4131 - acc: 0.8242\n",
            "Epoch 22: val_loss did not improve from 0.47243\n",
            "313/313 [==============================] - 29s 93ms/step - loss: 0.4131 - acc: 0.8242 - val_loss: 0.4779 - val_acc: 0.7772\n",
            "Epoch 23/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 0.4119 - acc: 0.8260Restoring model weights from the end of the best epoch: 19.\n",
            "\n",
            "Epoch 23: val_loss did not improve from 0.47243\n",
            "313/313 [==============================] - 30s 95ms/step - loss: 0.4119 - acc: 0.8260 - val_loss: 0.4746 - val_acc: 0.7792\n",
            "Epoch 23: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "sentences = [\n",
        "    '강감찬은 나를 너무 너무 좋아해',\n",
        "    '강감찬은 영화를 좋아해'\n",
        "]\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "# 단어 인덱스를 확인해 보아요!\n",
        "print(f'단어 인덱스는 : {tokenizer.word_index}')\n",
        "\n",
        "# 숫자로 변경하려면 어떻게 해야 하나요?\n",
        "word_encoding = tokenizer.texts_to_sequences(sentences)\n",
        "word_encoding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4cX0sIJDj9m",
        "outputId": "d06368c8-768a-49e4-c6fb-412c05e84c71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 인덱스는 : {'강감찬은': 1, '너무': 2, '좋아해': 3, '나를': 4, '영화를': 5}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 4, 2, 2, 3], [1, 5, 3]]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어 사전에 없는 새로운 단어가 등장하면?\n",
        "new_sentence = [\n",
        "    '강감찬은 신사임당을 너무 좋아해'\n",
        "]\n",
        "new_word_encoding = tokenizer.texts_to_sequences(new_sentence)\n",
        "new_word_encoding\n",
        "\n",
        "# 기본적인 동작은 이렇게 단어사전에 없는 데이터는 변환이 안되기 때문에\n",
        "# 그냥 없애버려요!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8krM5cuakC7",
        "outputId": "b13f6046-7107-4991-a29f-dbcd47088cef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 2, 3]]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 이렇게 처리하면 곤란해요\n",
        "# OOV(Out Of Vocabulary) token은 따로 처리해줘야 해요\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "sentences = [\n",
        "    '강감찬은 나를 너무 너무 좋아해',\n",
        "    '강감찬은 영화를 좋아해'\n",
        "]\n",
        "tokenizer = Tokenizer(oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "# 단어 인덱스를 확인해 보아요!\n",
        "print(f'단어 인덱스는 : {tokenizer.word_index}')\n",
        "\n",
        "# 숫자로 변경하려면 어떻게 해야 하나요?\n",
        "word_encoding = tokenizer.texts_to_sequences(sentences)\n",
        "word_encoding\n",
        "\n",
        "# # 새로운 단어 처리 ver.\n",
        "# new_sentence = [\n",
        "#     '강감찬은 신사임당을 너무 좋아해'\n",
        "# ]\n",
        "# new_word_encoding = tokenizer.texts_to_sequences(new_sentence)\n",
        "# new_word_encoding\n",
        "\n",
        "# 이제 길이도 맞춰야 해요!\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "padding = pad_sequences(word_encoding, maxlen=4)\n",
        "padding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7W7q53mcko2",
        "outputId": "9638652d-2264-4117-80b0-2ac117a5f0ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 인덱스는 : {'<OOV>': 1, '강감찬은': 2, '너무': 3, '좋아해': 4, '나를': 5, '영화를': 6}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5, 3, 3, 4],\n",
              "       [0, 2, 6, 4]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 위처럼 vocabulary를 구성하면 모든 token들이 포함되는 단어사전이 만들어져요\n",
        "# but, 단어사전이 너무 커진다는 단점 존재\n",
        "# 일반적으로 빈도수가 높은 단어들만 단어사전에 포함\n",
        "# 빈도수가 높은 순서로 최대 사전개수 정하고 빈도수가 작은 단어 제외하면 된다\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "sentences = [\n",
        "    '강감찬은 나를 너무 너무 좋아해',\n",
        "    '강감찬은 영화를 좋아해'\n",
        "]\n",
        "tokenizer = Tokenizer(num_words=3,  # 해당 숫자 미만의 값들만 표현!\n",
        "                      oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "# 단어 인덱스를 확인해 보아요!\n",
        "print(f'단어 인덱스는 : {tokenizer.word_index}')\n",
        "\n",
        "# 숫자로 변경하려면 어떻게 해야 하나요?\n",
        "word_encoding = tokenizer.texts_to_sequences(sentences)\n",
        "word_encoding\n",
        "\n",
        "# 새로운 단어 처리 ver.\n",
        "new_sentence = [\n",
        "    '강감찬은 신사임당을 너무 좋아해'\n",
        "]\n",
        "new_word_encoding = tokenizer.texts_to_sequences(new_sentence)\n",
        "new_word_encoding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1CZoV-scdbRf",
        "outputId": "08460106-56fb-4437-b40e-d709ff8e7261"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 인덱스는 : {'<OOV>': 1, '강감찬은': 2, '너무': 3, '좋아해': 4, '나를': 5, '영화를': 6}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[2, 1, 1, 1]]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 기본적인 내용을 알아봤으니 이제 실제로 구현 해보아요!\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "# Naver가 제공하는 네이버 영화 리뷰 데이터 로딩\n",
        "train_file = tf.keras.utils.get_file(\n",
        "    '/content/ratings_train.txt',  # 파일 저장 경로\n",
        "    origin = 'https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt',  # origin => 데이터 읽어올 경로\n",
        "    extract=True\n",
        ")\n",
        "\n",
        "train_df = pd.read_csv(train_file, sep='\\t')  # sep -> 구분자 설정, tab으로 구분되어 있으므로 \\t\n",
        "train_df.head()\n",
        "train_df.shape\n",
        "\n",
        "test_file = tf.keras.utils.get_file(\n",
        "    '/content/ratings_test.txt',  # 파일 저장 경로\n",
        "    origin = 'https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt',  # origin => 데이터 읽어올 경로\n",
        "    extract=True\n",
        ")\n",
        "test_df = pd.read_csv(test_file, sep='\\t')\n",
        "test_df.head()\n",
        "test_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVExEaTuexd3",
        "outputId": "8ef0ce73-3863-476a-d825-7c19c3a0f6b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 간단하게 데이터 확인 과정만 해 보아요!\n",
        "# 긍정과 부정 비율 확인\n",
        "cnt = train_df['label'].value_counts()\n",
        "cnt\n",
        "\n",
        "# 결측치 확인\n",
        "# 댓글에 5개 결측치 존재 - 추후 삭제 후 진행\n",
        "train_df.isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2OkZ5N3hxhp",
        "outputId": "a927c475-13f8-42c0-8786-602aca954957"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id          0\n",
              "document    5\n",
              "label       0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 형태소 분석해야\n",
        "# 다양한 한국어 형태소 분석기 존재\n",
        "# Kkma, Komoran, Okt, Mecab 등이 있으며\n",
        "# 어떤 형태소 분석기 사용하느냐에 따라 결과 달라짐\n",
        "# koNLPy라는 라이브러리 이용해서 사용\n",
        "# Colab의 경우, 간단히 해당 라이브러리만 설치하면 된다\n",
        "!pip install konlpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHtMNMZSnUyG",
        "outputId": "811f2642-f456-481e-c0f9-db052c21d966"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading JPype1-1.4.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (465 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m465.3/465.3 kB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.3)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (23.2)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.4.1 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Okt, Komoran, Kkma\n",
        "\n",
        "kkma = Kkma()\n",
        "okt = Okt()\n",
        "komoran = Komoran()\n",
        "\n",
        "text = '이것은 소리없는 아우성. 저 푸른 해원을 향하여 흔드는 노스텔지어의 손수건'\n",
        "kkma.morphs(text)  # morphs 함수 사용하면 형태소 추출 가능\n",
        "okt.morphs(text)\n",
        "komoran.morphs(text)"
      ],
      "metadata": {
        "id": "YqCGw9bao5ds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 전처리도 진행해야 해요\n",
        "\n",
        "# 1. 댓글 데이터에는 특수문자, 숫자 같은게 함께 포함되어 있어요\n",
        "#    정규 표현식 사용해서 영어, 한글, 띄어쓰기만 남기고 나머지는 제거\n",
        "\n",
        "# 2. 결측치 제거\n",
        "# 3. stopword(불용어)를 제거\n",
        "# 4. 단어사전 만들고 숫자로 변경하는 token화 작업 진행\n",
        "# 5. 동일한 문장 길이로 정리(paading 처리)\n",
        "\n",
        "#########################################\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "# Naver가 제공하는 네이버 영화 리뷰 데이터 로딩\n",
        "train_file = tf.keras.utils.get_file(\n",
        "    '/content/ratings_train.txt',  # 파일 저장 경로\n",
        "    origin = 'https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt',  # origin => 데이터 읽어올 경로\n",
        "    extract=True\n",
        ")\n",
        "train_df = pd.read_csv(train_file, sep='\\t')  # sep -> 구분자 설정, tab으로 구분되어 있으므로 \\t\n",
        "\n",
        "test_file = tf.keras.utils.get_file(\n",
        "    '/content/ratings_test.txt',  # 파일 저장 경로\n",
        "    origin = 'https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt',  # origin => 데이터 읽어올 경로\n",
        "    extract=True\n",
        ")\n",
        "test_df = pd.read_csv(test_file, sep='\\t')\n",
        "\n",
        "# [^A-Za-z가-힣ㄱ-ㅎㅏ-ㅣ ] => ^은 not을 의미(영어, 한글, 공백이 아닌 것 의미)\n",
        "# 1. 특수문자 제거(정규을 이용!)\n",
        "train_df['document'] = train_df['document'].str.replace('[^A-Za-z가-힣ㄱ-ㅎㅏ-ㅣ ]','')\n",
        "train_df.head()\n",
        "\n",
        "# 2. 결측치 제거(5개)\n",
        "train_df = train_df.dropna()\n",
        "\n",
        "# 3. stop word(불용어) 처리\n",
        "# 관사, 조치사, 조사, 접속사등 의미가 없는 단어들 의미\n",
        "# 한글에는 이런 불용어가 굉장히 많다 -> 전부 다 처리하기는 불가능하므로\n",
        "# 자주 사용하는 몇 개만 처리할 예정\n",
        "from konlpy.tag import Kkma\n",
        "okt = Okt()\n",
        "def word_tokenization(text):\n",
        "    stop_words = ['는', '을', '를', '이', '가', '의', '던', '고', '하', '다',\n",
        "                  '은', '에', '들', '지', '게', '도']\n",
        "    return [word for word in okt.morphs(text) if word not in stop_words]\n",
        "\n",
        "data_train = train_df['document'].apply(lambda x: word_tokenization(x))\n",
        "data_train.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ltkfAszqN8-",
        "outputId": "b5d2ec56-ab8f-428e-fd74-df354df2dd6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-25-bcce9b3fa27f>:34: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  train_df['document'] = train_df['document'].str.replace('[^A-Za-z가-힣ㄱ-ㅎㅏ-ㅣ ]','')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 불용어 처리가 됬다고 가정하고 진행\n",
        "# 단어 사전을 만들고 문자를 숫자로 변경하는 token화 진행!\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(data_train)  # 단어사전 생성!\n",
        "\n",
        "# 일단 총 단어의 개수 확인\n",
        "print(f'총 단어의 개수는 : {len(tokenizer.word_index)}')\n",
        "\n",
        "# 단어사전 출력하기\n",
        "tokenizer.word_index\n",
        "\n",
        "# 각 토큰이 나타난 수(빈도)를 알아보아요!\n",
        "tokenizer.word_counts\n",
        "\n",
        "# 각 토큰이 나타난 수(빈도)에서 숫자만 출력\n",
        "tokenizer.word_counts.values()\n",
        "\n",
        "# 단어 빈도수 5회 이상인 단어가 몇개인 지 확인\n",
        "def get_voca_size(threshold):\n",
        "    count = 0\n",
        "    for x in tokenizer.word_counts.values():\n",
        "        if x > threshold:\n",
        "            count += 1\n",
        "    return count\n",
        "\n",
        "voca_size = get_voca_size(5)"
      ],
      "metadata": {
        "id": "ujs-AXjWszHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어사전을 생성\n",
        "tokenizer = Tokenizer(oov_token='<OOV>',\n",
        "                      num_words=15000)\n",
        "tokenizer.fit_on_texts(data_train)\n",
        "\n",
        "# 이제 각 문장을 숫자 벡터로 변환하여 encoding\n",
        "data_train_seq = tokenizer.texts_to_sequences(data_train)\n",
        "\n",
        "# 동일한 길이로 문장 길이를 정리 해야해요\n",
        "max_length = max([len(x) for x in data_train_seq])\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "x_data_train = pad_sequences(data_train_seq,\n",
        "                             maxlen=max_length)\n",
        "\n",
        "t_data_train = train_df['label'].values"
      ],
      "metadata": {
        "id": "T3EK-_df5U-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding, Bidirectional  # Bidirectional - 양방향 RNN 만들기 위해 사용\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(input_dim=15000,\n",
        "                    output_dim=32,\n",
        "                    input_length=71))\n",
        "\n",
        "# 양방향 LSTM을 만들어요!()\n",
        "model.add(Bidirectional(LSTM(units=16,\n",
        "                             activation='tanh')))\n",
        "\n",
        "model.add(Dense(units=32,\n",
        "                activation='relu'))\n",
        "\n",
        "model.add(Dense(units=1,\n",
        "                activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=1e-4),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['acc'])\n",
        "\n",
        "# Callback 설정\n",
        "cp_cb = ModelCheckpoint(filepath='./best_model.ckpt',\n",
        "                        save_weights_only=True,\n",
        "                        save_best_only=True,\n",
        "                        monitor='val_loss',\n",
        "                        verbose=1)\n",
        "es_cb = EarlyStopping(monitor='val_loss',\n",
        "                      patience=3,\n",
        "                      verbose=1)\n",
        "\n",
        "# 학습\n",
        "history = model.fit(x_data_train,\n",
        "                    t_data_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=64,\n",
        "                    validation_split=0.2,\n",
        "                    callbacks=[cp_cb, es_cb],\n",
        "                    verbose=1)\n"
      ],
      "metadata": {
        "id": "XFLzM6Xb771g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습이 끝난 다음 prediction을 할 수 있어요\n",
        "# 예측 데이터는 다음과 같은 데이터 사용\n",
        "review_sentences = ['내가 만들어도 이것보단 잘 만들겠다.',\n",
        "                    '너무너무 재미있었습니다. 감사합니다.',\n",
        "                    '아..내 돈... 돈 아까워 죽을거 같아요',\n",
        "                    '아나..이것도 영화라고 만들었냐. 무슨 스토리가 산으로 가냐',\n",
        "                    '감동과 재미가 같이 있는 영화입니다. 훌륭합니다.',\n",
        "                    '너무너무 재미없다.. 잠와 죽는줄.',\n",
        "                    '너무너무 재미있다.. 잠이 확깨네.']"
      ],
      "metadata": {
        "id": "rOFcwSJ3-2Or"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}